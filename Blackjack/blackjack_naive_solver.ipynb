{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BLACKJACK NAIVE AGENT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook we will implement a naive agent for the Blackjack environment. The agent will be able to play Blackjack, but it will not learn anything. We will use this agent to understand the Blackjack environment and to test our implementation of the environment. Its policy will be to stick if the sum of the cards is 20 or 21, and to hit otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZGV9uY3Gy5SS",
        "outputId": "8b6e65cd-b020-4f10-c8e8-41ee24da0574"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n%pip install wandb\\n%pip install matplotlib\\n%pip install numpy\\n%pip install tqdm\\n%matplotlib inline\\n%pip install gymnasium==0.29.1\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Uncomment the following lines to install the required packages\n",
        "\n",
        "'''\n",
        "%pip install wandb\n",
        "%pip install matplotlib\n",
        "%pip install numpy\n",
        "%pip install tqdm\n",
        "%matplotlib inline\n",
        "%pip install gymnasium==0.29.1\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bwXFNMMJyqga"
      },
      "outputs": [],
      "source": [
        "#Importing the required packages\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy-xWY1nyqgc"
      },
      "source": [
        "Let´s first of all create the environment.\n",
        "We´ll use the Gymnasium´s Blackjack environment, we´ll allow natural blackjacks as well and the settings won´t follow the Sutton & Barto´s Book´s approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yGmg78jOyqgd"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Blackjack-v1',sab=False, natural=True, render_mode='rgb_array') #We are not folllowing the default sutton and barto book settings, which are sab=True, natural=False, render_mode='human'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z21vH9lByqgd"
      },
      "source": [
        "### Understanding and Observing the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSnEO4jayqgd",
        "outputId": "6259ec67-1171-44ca-c640-d661a1c03e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "\n",
            "Action space: Discrete(2)\n",
            "\n",
            "Observation: (7, 2, 0)\n",
            "\n",
            " info: {}\n"
          ]
        }
      ],
      "source": [
        "#observation space is a tuple of 3 elements:\n",
        "#1. player's current sum (1-31)\n",
        "#2. dealer's face up card (1-10)\n",
        "#3. whether or not the player has a usable ace (0 or 1)\n",
        "\n",
        "done = False\n",
        "observation, info = env.reset() #get the first observation\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"\\nAction space:\", env.action_space) #0: stick, 1: hit\n",
        "print(\"\\nObservation:\", observation) #Observation[1] is player's current sum, Observation[2] is dealer's face up card, Observation[3] is whether or not the player has a usable ace\n",
        "print(\"\\n info:\", info)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ4zC26Kyqge"
      },
      "source": [
        "### Now let´s see how the agent behaves when making a step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imLtQXJGyqge"
      },
      "source": [
        "**env.step(action)** returns: observation, reward, terminated, truncated, info\n",
        "\n",
        "**observation**: tuple of 3 elements (player's current sum, dealer's face up card, whether or not the player has a usable ace)\n",
        "\n",
        "**reward**: +1.5, +1, 0 or -1 (win, draw or loss), 1.5 if the player wins with a natural blackjack\n",
        "\n",
        "**terminated**: boolean (True if the episode is over)\n",
        "\n",
        "**truncated**: boolean (True if the episode is over because it reached the maximum number of steps)\n",
        "\n",
        "**info**: dictionary with additional information. We will not use this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMepm6TLyqge",
        "outputId": "93da7d54-127a-46c7-a6f6-622596fe333e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random actions:\n",
            "Action: 1\n",
            "Observation: (19, 10, 0)\n",
            "Reward: 0.0\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n",
            "Action: 1\n",
            "Observation: (23, 7, 0)\n",
            "Reward: -1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n",
            "Action: 0\n",
            "Observation: (7, 10, 0)\n",
            "Reward: -1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n",
            "Action: 0\n",
            "Observation: (20, 10, 0)\n",
            "Reward: 1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n",
            "Action: 1\n",
            "Observation: (11, 6, 0)\n",
            "Reward: 0.0\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#sample random actions from the action space\n",
        "print(\"Random actions:\")\n",
        "for i in range(5):\n",
        "    env.reset() # reset the environment at the beginning of each iteration\n",
        "    action = env.action_space.sample()\n",
        "    print(\"Action:\", action)\n",
        "    observation, reward, terminated, truncated, info = env.step(action) #take a random action and observe the results of the action taken\n",
        "    print(\"Observation:\", observation) #Observation[1] is player's current sum, Observation[2] is dealer's face up card, Observation[3] is whether or not the player has a usable ace\n",
        "    print(\"Reward:\", reward) #reward is 1 if the player wins, 1.5 if player wins with natural blackjack (an usable ace and a 10), -1 if the player loses, and 0 if the game is a draw\n",
        "    print(\"Terminated:\", terminated)\n",
        "    print(\"Truncated:\", truncated)\n",
        "    print(\"info:\", info)\n",
        "    print(\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGoWlDIIyqgf"
      },
      "source": [
        "Let´s create a simple agent, the policy is very naive, if its own sum surpasses 20, sticks with its cards, if not, hits for more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-L7mSdRNyqgf"
      },
      "outputs": [],
      "source": [
        "class NaiveBlackjackAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def play(self, obs):\n",
        "        return 0 if obs[0] >= 20 else 1 #stick if player's current sum is 20 or more, else hit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__wluxQhyqgf"
      },
      "source": [
        "Now we will evaluate the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AJ6OITW-yqgf"
      },
      "outputs": [],
      "source": [
        "#defining the hyperparameters\n",
        "n_episodes = 100\n",
        "\n",
        "#initialize the agent\n",
        "agent = NaiveBlackjackAgent()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6-g1ahH21ASn",
        "outputId": "315441cd-f5e6-43bf-d212-d3542f90b9b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 2052/100000 [03:02<1:32:01, 17.74it/s]"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "import wandb\n",
        "import torch\n",
        "import pygame\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"blackjack_naive_100000\", entity=\"ai42\")\n",
        "pygame.init()\n",
        "\n",
        "\n",
        "n_episodes = 100000  # Define the number of episodes you want to run\n",
        "\n",
        "\n",
        "win_rate = 0.0\n",
        "loss_rate = 0.0\n",
        "draw_rate = 0.0\n",
        "natural_rate = 0.0\n",
        "\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    obs, info = env.reset()\n",
        "    terminated, truncated = False, False\n",
        "    clear_output(wait=True)\n",
        "    step = 0\n",
        "    episode_rewards = 0  # Initialize total rewards for the episode\n",
        "\n",
        "    while not terminated and not truncated:\n",
        "        action = agent.play(obs)  # Agent's policy\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # Ensure you're getting an RGB image\n",
        "        frame = env.render()\n",
        "        step += 1\n",
        "        episode_rewards += reward  # Accumulate rewards\n",
        "\n",
        "        # Plot frame\n",
        "        plt.imshow(frame)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Episode: {episode}, Step: {step}\")\n",
        "        plt.savefig('frame.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Log the frame and rewards to wandb\n",
        "        wandb.log({\n",
        "            \"episode\": episode,\n",
        "            \"step\": step,\n",
        "            \"frame\": wandb.Image('frame.png'),\n",
        "            \"reward\": reward,\n",
        "            \"cumulative_reward\": episode_rewards\n",
        "        })\n",
        "    if reward == 1 or reward == 1.5:\n",
        "        win_rate += 1\n",
        "    elif reward == -1:\n",
        "        loss_rate += 1\n",
        "    elif reward == 0:\n",
        "        draw_rate += 1\n",
        "    if reward == 1.5:\n",
        "        natural_rate += 1\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Let´s log general statistics of the training\n",
        "wandb.log({\"Win_rate\": win_rate / n_episodes, \"Loss_rate\": loss_rate / n_episodes, \"Draw_rate\": draw_rate / n_episodes, \"Natural_win_rate\": natural_rate / n_episodes}) # Log the episode statistics to wandb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
