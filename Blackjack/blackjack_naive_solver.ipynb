{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "%pip install wandb\n",
        "%pip install matplotlib\n",
        "%pip install numpy\n",
        "%pip install tqdm\n",
        "%matplotlib inline\n",
        "%pip install gymnasium==0.29.1\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZGV9uY3Gy5SS",
        "outputId": "8b6e65cd-b020-4f10-c8e8-41ee24da0574"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n%pip install wandb\\n%pip install matplotlib\\n%pip install numpy\\n%pip install tqdm\\n%matplotlib inline\\n%pip install gymnasium==0.29.1\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bwXFNMMJyqga"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from collections import defaultdict #for accessing keys which are not present in dictionary\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import random\n",
        "from matplotlib.patches import Patch\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy-xWY1nyqgc"
      },
      "source": [
        "Let´s first of all create the environment.\n",
        "We´ll use the Gymnasium´s Blackjack environment, we´ll allow natural blackjacks as well and the settings won´t follow the Sutton & Barto´s Book´s approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yGmg78jOyqgd"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Blackjack-v1',sab=False, natural=True, render_mode='rgb_array') #We are not folllowing the default sutton and barto book settings, which are sab=True, natural=False, render_mode='human'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z21vH9lByqgd"
      },
      "source": [
        "### Understanding and Observing the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSnEO4jayqgd",
        "outputId": "6259ec67-1171-44ca-c640-d661a1c03e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "\n",
            "Action space: Discrete(2)\n",
            "\n",
            "Observation: (16, 10, 0)\n",
            "\n",
            " info: {}\n"
          ]
        }
      ],
      "source": [
        "#observation space is a tuple of 3 elements:\n",
        "#1. player's current sum (1-31)\n",
        "#2. dealer's face up card (1-10)\n",
        "#3. whether or not the player has a usable ace (0 or 1)\n",
        "\n",
        "done = False\n",
        "observation, info = env.reset() #get the first observation\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"\\nAction space:\", env.action_space) #0: stick, 1: hit\n",
        "print(\"\\nObservation:\", observation) #Observation[1] is player's current sum, Observation[2] is dealer's face up card, Observation[3] is whether or not the player has a usable ace\n",
        "print(\"\\n info:\", info)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ4zC26Kyqge"
      },
      "source": [
        "### Now let´s see how the agent behaves when making a step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imLtQXJGyqge"
      },
      "source": [
        "**env.step(action)** returns: observation, reward, terminated, truncated, info\n",
        "\n",
        "**observation**: tuple of 3 elements (player's current sum, dealer's face up card, whether or not the player has a usable ace)\n",
        "\n",
        "**reward**: +1.5, +1, 0 or -1 (win, draw or loss), 1.5 if the player wins with a natural blackjack\n",
        "\n",
        "**terminated**: boolean (True if the episode is over)\n",
        "\n",
        "**truncated**: boolean (True if the episode is over because it reached the maximum number of steps)\n",
        "\n",
        "**info**: dictionary with additional information. We will not use this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMepm6TLyqge",
        "outputId": "93da7d54-127a-46c7-a6f6-622596fe333e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random actions:\n",
            "Action: 0\n",
            "Observation: (12, 5, 0)\n",
            "Reward: 1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n",
            "Action: 0\n",
            "Observation: (9, 5, 0)\n",
            "Reward: 1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n",
            "Action: 1\n",
            "Observation: (23, 10, 0)\n",
            "Reward: -1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n",
            "Action: 1\n",
            "Observation: (23, 8, 0)\n",
            "Reward: -1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n",
            "Action: 1\n",
            "Observation: (21, 9, 0)\n",
            "Reward: 0.0\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "info: {}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#sample random actions from the action space\n",
        "print(\"Random actions:\")\n",
        "for i in range(5):\n",
        "    env.reset() # reset the environment at the beginning of each iteration\n",
        "    action = env.action_space.sample()\n",
        "    print(\"Action:\", action)\n",
        "    observation, reward, terminated, truncated, info = env.step(action) #take a random action and observe the results of the action taken\n",
        "    print(\"Observation:\", observation) #Observation[1] is player's current sum, Observation[2] is dealer's face up card, Observation[3] is whether or not the player has a usable ace\n",
        "    print(\"Reward:\", reward) #reward is 1 if the player wins, 1.5 if player wins with natural blackjack (an usable ace and a 10), -1 if the player loses, and 0 if the game is a draw\n",
        "    print(\"Terminated:\", terminated)\n",
        "    print(\"Truncated:\", truncated)\n",
        "    print(\"info:\", info)\n",
        "    print(\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGoWlDIIyqgf"
      },
      "source": [
        "Let´s create a simple agent, the policy is very naive, if its own sum surpasses 20, sticks with its cards, if not, hits for more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-L7mSdRNyqgf"
      },
      "outputs": [],
      "source": [
        "class NaiveBlackjackAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def play(self, obs):\n",
        "        return 0 if obs[0] >= 20 else 1 #stick if player's current sum is 20 or more, else hit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__wluxQhyqgf"
      },
      "source": [
        "Now we will evaluate the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AJ6OITW-yqgf"
      },
      "outputs": [],
      "source": [
        "#defining the hyperparameters\n",
        "n_episodes = 100\n",
        "\n",
        "#initialize the agent\n",
        "agent = NaiveBlackjackAgent()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "vzl8QESYyqgf",
        "outputId": "9e54e1be-f4fa-410b-c944-204395ce780f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from collections import deque\\nfrom gymnasium.wrappers import RecordEpisodeStatistics\\nfrom IPython.display import clear_output\\nimport wandb\\nimport pygame\\n\\n\\n# initialize pygame and wandb\\npygame.init()\\nwandb.init(project=\"blackjack_naive\", entity=\"ai42\")\\n\\n# Assuming env and agent are defined and initialized here\\n\\nfor episode in tqdm(range(n_episodes)):\\n    obs, info = env.reset()\\n    terminated = False\\n    truncated = False\\n    clear_output()\\n    step = 0\\n    \\n    while not terminated and not truncated:\\n        action = agent.play(obs)  # Agent\\'s policy\\n        obs, reward, terminated, truncated, info = env.step(action)\\n        \\n        frame = env.render() # Ensure you\\'re getting an RGB image\\n        step += 1\\n        plt.imshow(frame)\\n        plt.axis(\\'off\\')\\n        plt.title(f\"Episode: {episode}, Step: {step}\")\\n        plt.savefig(\\'frame.png\\')\\n        if terminated or truncated:\\n            plt.title(f\"TERMINATED OR TRUNCATED, Episode: {episode}, Step: {step}\")\\n            plt.savefig(\\'frame.png\\')\\n            wandb.log({\"frame\": wandb.Image(\\'frame.png\\')})\\n            plt.close()\\n            break\\n        wandb.log({\"frame\": wandb.Image(\\'frame.png\\')})\\n        plt.close()\\n\\n        \\n\\n    print(\"Reward:\", reward)\\n    print(\"Done:\", terminated or truncated)\\n    print(\"info\", info)\\n    wandb.log({\"reward\": reward})\\n    print(\"\")\\n\\nenv.close()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "'''from collections import deque\n",
        "from gymnasium.wrappers import RecordEpisodeStatistics\n",
        "from IPython.display import clear_output\n",
        "import wandb\n",
        "import pygame\n",
        "\n",
        "\n",
        "# initialize pygame and wandb\n",
        "pygame.init()\n",
        "wandb.init(project=\"blackjack_naive\", entity=\"ai42\")\n",
        "\n",
        "# Assuming env and agent are defined and initialized here\n",
        "\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    obs, info = env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    clear_output()\n",
        "    step = 0\n",
        "\n",
        "    while not terminated and not truncated:\n",
        "        action = agent.play(obs)  # Agent's policy\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        frame = env.render() # Ensure you're getting an RGB image\n",
        "        step += 1\n",
        "        plt.imshow(frame)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Episode: {episode}, Step: {step}\")\n",
        "        plt.savefig('frame.png')\n",
        "        if terminated or truncated:\n",
        "            plt.title(f\"TERMINATED OR TRUNCATED, Episode: {episode}, Step: {step}\")\n",
        "            plt.savefig('frame.png')\n",
        "            wandb.log({\"frame\": wandb.Image('frame.png')})\n",
        "            plt.close()\n",
        "            break\n",
        "        wandb.log({\"frame\": wandb.Image('frame.png')})\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Reward:\", reward)\n",
        "    print(\"Done:\", terminated or truncated)\n",
        "    print(\"info\", info)\n",
        "    wandb.log({\"reward\": reward})\n",
        "    print(\"\")\n",
        "\n",
        "env.close()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "from gymnasium.wrappers import RecordEpisodeStatistics\n",
        "from IPython.display import clear_output\n",
        "import wandb\n",
        "import pygame\n",
        "\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"blackjack_naive_100000\", entity=\"ai42\")\n",
        "pygame.init()\n",
        "\n",
        "\n",
        "n_episodes = 100000  # Define the number of episodes you want to run\n",
        "\n",
        "\n",
        "win_rate = 0.0\n",
        "loss_rate = 0.0\n",
        "draw_rate = 0.0\n",
        "natural_rate = 0.0\n",
        "\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    obs, info = env.reset()\n",
        "    terminated, truncated = False, False\n",
        "    clear_output(wait=True)\n",
        "    step = 0\n",
        "    episode_rewards = 0  # Initialize total rewards for the episode\n",
        "\n",
        "    while not terminated and not truncated:\n",
        "        action = agent.play(obs)  # Agent's policy\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # Ensure you're getting an RGB image\n",
        "        frame = env.render()\n",
        "        step += 1\n",
        "        episode_rewards += reward  # Accumulate rewards\n",
        "\n",
        "        # Plot frame\n",
        "        plt.imshow(frame)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Episode: {episode}, Step: {step}\")\n",
        "        plt.savefig('frame.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Log the frame and rewards to wandb\n",
        "        wandb.log({\n",
        "            \"episode\": episode,\n",
        "            \"step\": step,\n",
        "            \"frame\": wandb.Image('frame.png'),\n",
        "            \"reward\": reward,\n",
        "            \"cumulative_reward\": episode_rewards\n",
        "        })\n",
        "    if reward == 1 or reward == 1.5:\n",
        "        win_rate += 1\n",
        "    elif reward == -1:\n",
        "        loss_rate += 1\n",
        "    elif reward == 0:\n",
        "        draw_rate += 1\n",
        "    if reward == 1.5:\n",
        "        natural_rate += 1\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Let´s log general statistics of the training\n",
        "wandb.log({\"Win_rate\": win_rate / n_episodes, \"Loss_rate\": loss_rate / n_episodes, \"Draw_rate\": draw_rate / n_episodes, \"Natural_win_rate\": natural_rate / n_episodes}) # Log the episode statistics to wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "6-g1ahH21ASn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "315441cd-f5e6-43bf-d212-d3542f90b9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 582/100000 [04:08<11:50:26,  2.33it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tN9dRXCKV08N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}