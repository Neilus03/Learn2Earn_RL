{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (1.32.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (4.7.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from matplotlib) (5.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (1.25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium==0.29.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from gymnasium==0.29.1) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from gymnasium==0.29.1) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from gymnasium==0.29.1) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from gymnasium==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from gymnasium==0.29.1) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\neild\\miniconda3\\envs\\pytorch-env\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium==0.29.1) (3.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install tqdm\n",
    "%matplotlib inline\n",
    "%pip install gymnasium==0.29.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "from collections import defaultdict #for accessing keys which are not present in dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import random\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_BlackjackAgent:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon #exploration rate\n",
    "        self.possible_actions = [0,1] #0 for stick, 1 for hit\n",
    "\n",
    "    def play(self, obs, best_action): #best_action is the action with highest Q value\n",
    "        \n",
    "        if obs[0] < 12: #if sum of player's cards is less than 12 then always hit \n",
    "            return 1 #hit\n",
    "\n",
    "        elif obs[0] >= 12 and obs[0] < 21: #if sum of player's cards is between 12 and 21\n",
    "            if np.random.random() > self.epsilon:\n",
    "                action_to_take = best_action\n",
    "            else:\n",
    "                action_to_take = random.choice(self.possible_actions)\n",
    "            return action_to_take\n",
    "        \n",
    "        else: #if sum of player's cards is greater or equal than 21 then always stick\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.15\n",
    "agent = MC_BlackjackAgent(epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [49:03<00:00,  3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "from IPython.display import clear_output\n",
    "import wandb\n",
    "import pygame\n",
    "from numpy import random\n",
    "\n",
    "#load the environment\n",
    "env = gym.make('Blackjack-v1',sab=False, natural=True, render_mode='rgb_array') #We are not folllowing the default sutton and barto book settings, which are sab=True, natural=False, render_mode='human'\n",
    "\n",
    "possible_states = [(i, j, k) for i in range(12, 21) for j in range(1, 11) for k in range(2)] #possible states are (player_sum, dealer_card, usable_ace)\n",
    "possible_actions = [0, 1] #possible actions are (stick, hit)\n",
    "\n",
    "#dictionary for storing the action value functions for each state\n",
    "dict_state_action_value_functions = {state: \n",
    "                                        {action: \n",
    "                                            {\"return_sum\": 0, \"count\":0, 'running_expected_return': 0}\n",
    "                                        for action in possible_actions} \n",
    "                                    for state in possible_states} \n",
    "\n",
    "best_action = random.choice([0,1]) #best_action is the action with highest Q value, by default is random (only for the first iteration)\n",
    "\n",
    "for _ in range(1):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"blackjack_MC_First_Visit\", entity=\"ai42\")\n",
    "    pygame.init()\n",
    "    \n",
    "    n_episodes = 10000  # Define the number of episodes you want to run\n",
    "\n",
    "    wins = 0.0\n",
    "    losses = 0.0\n",
    "    draws = 0.0\n",
    "    naturals = 0.0\n",
    "\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        obs, info = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        clear_output()\n",
    "        step = 0\n",
    "        episode_rewards = 0  # Initialize total rewards for the episode\n",
    "\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action = agent.play(obs, best_action)  # Agent's policy\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            if obs[0] >= 12 and obs[0] < 21: #actually policy only affects if sum of player's cards is between 12 and 21, then, restricting this condition we can reduce the number of iterations and make the training faster\n",
    "                dict_state_action_value_functions[obs][action][\"count\"] += 1 #increment the count of the state-action pair\n",
    "                dict_state_action_value_functions[obs][action][\"return_sum\"] += reward #increment the return sum of the state-action pair\n",
    "                dict_state_action_value_functions[obs][action][\"running_expected_return\"] = dict_state_action_value_functions[obs][action][\"return_sum\"] / dict_state_action_value_functions[obs][action][\"count\"] #calculate the running expected return of the state-action pair\n",
    "                \n",
    "                best_action = np.argmax([dict_state_action_value_functions[obs][action][\"running_expected_return\"] for action in possible_actions]) #best_action is the action with highest Q value\n",
    "            \n",
    "            frame = env.render() # Render the frame\n",
    "            step += 1 # Increment the step counter\n",
    "            episode_rewards += reward  # Accumulate rewards\n",
    "\n",
    "            # Plot frame\n",
    "            plt.imshow(frame)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Episode: {episode} - Step: {step} - Action Taken: {action} - Reward: {reward} - Terminated: {terminated}\")\n",
    "            plt.savefig('frame.png')\n",
    "            plt.close()\n",
    "\n",
    "            \n",
    "            \n",
    "            # Log the frame and rewards to wandb\n",
    "            wandb.log({\n",
    "                \"episode\": episode,\n",
    "                \"step\": step,\n",
    "                \"frame\": wandb.Image('frame.png'),\n",
    "                \"reward\": reward,\n",
    "                \"cumulative_reward\": episode_rewards,\n",
    "            })\n",
    "        if reward == 1 or reward == 1.5:\n",
    "            wins += 1\n",
    "        elif reward == -1:\n",
    "            losses += 1\n",
    "        elif reward == 0:\n",
    "            draws += 1\n",
    "        if reward == 1.5:\n",
    "            naturals += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Let´s log general statistics of the training\n",
    "    wandb.log({\"Win_rate\": wins / n_episodes, \"Loss_rate\": losses / n_episodes, \"Draw_rate\": draws / n_episodes, \"Natural_win_rate\": naturals / n_episodes, \"epsilon\": epsilon}) # Log the episode statistics to wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
