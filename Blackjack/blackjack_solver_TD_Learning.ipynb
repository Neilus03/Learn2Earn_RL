{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%pip install gymnasium==0.27.0\\n%pip install matplotlib\\n%pip install numpy\\n%pip install tqdm\\n%matplotlib inline\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%pip install gymnasium==0.27.0\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install tqdm\n",
    "%matplotlib inline\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict #for accessing keys which are not present in dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import random\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1', sab=False, natural=True, render_mode='rgb_array') #We are not folllowing the default sutton and barto book settings, which are sab=True, natural=False, render_mode='human'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Action space: Discrete(2)\n",
      "Observation: (17, 10, False)\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "#observation space is a tuple of 3 elements:\n",
    "#1. player's current sum (1-31)\n",
    "#2. dealer's face up card (1-10)\n",
    "#3. whether or not the player has a usable ace (0 or 1)\n",
    "\n",
    "\n",
    "done = False\n",
    "observation, info = env.reset() #get the first observation\n",
    "print(\"Observation space:\", env.observation_space) \n",
    "print(\"Action space:\", env.action_space) #0: stick, 1: hit\n",
    "print(\"Observation:\", observation) #player´s first two cards\n",
    "print(\"Info:\", info) #dealer´s first card\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.step(action) returns: observation, reward, terminated, truncated, info\n",
    "\n",
    "#observation: tuple of 3 elements (player's current sum, dealer's face up card, whether or not the player has a usable ace)\n",
    "\n",
    "#reward: +1.5, +1, 0 or -1 (win, draw or loss), 1.5 if the player wins with a natural blackjack\n",
    "\n",
    "#terminated: boolean (True if the episode is over)\n",
    "\n",
    "#truncated: boolean (True if the episode is over because it reached the maximum number of steps)\n",
    "\n",
    "#info: dictionary with additional information. We will not use this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random actions:\n",
      "Action: 1\n",
      "Observation: (15, 10, False)\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "\n",
      "Action: 0\n",
      "Observation: (19, 10, True)\n",
      "Reward: 1.0\n",
      "Terminated: True\n",
      "Truncated: False\n",
      "Info: {}\n",
      "\n",
      "Action: 0\n",
      "Observation: (14, 10, False)\n",
      "Reward: -1.0\n",
      "Terminated: True\n",
      "Truncated: False\n",
      "Info: {}\n",
      "\n",
      "Action: 1\n",
      "Observation: (29, 5, False)\n",
      "Reward: -1.0\n",
      "Terminated: True\n",
      "Truncated: False\n",
      "Info: {}\n",
      "\n",
      "Action: 1\n",
      "Observation: (15, 1, False)\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neild\\Miniconda3\\envs\\pytorch-env\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "#sample random actions from the action space\n",
    "print(\"Random actions:\")\n",
    "for i in range(5):\n",
    "    env.reset() # reset the environment at the beginning of each iteration\n",
    "    action = env.action_space.sample()\n",
    "    print(\"Action:\", action)\n",
    "    observation, reward, terminated, truncated, info = env.step(action) #take a random action and observe the results of the action taken\n",
    "    print(\"Observation:\", observation)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Terminated:\", terminated)\n",
    "    print(\"Truncated:\", truncated)\n",
    "    print(\"Info:\", info)\n",
    "    print(\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon Greedy Strategy to solve blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon:float,\n",
    "        learning_rate:float,\n",
    "        initial_epsilon:float,\n",
    "        epsilon_decay:float,\n",
    "        final_epsilon:float,\n",
    "        discount_factor:float = 0.95,\n",
    "    ):\n",
    "        #initialize the agent's parameters with empty state-action value (q_values),\n",
    "        #a learning rate, an initial epsilon, an epsilon decay, a final epsilon and a discount factor\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        \n",
    "        self.epsilon = epsilon #epsilon value\n",
    "        self.lr = learning_rate #learning rate\n",
    "        self.initial_epsilon = initial_epsilon #initial value of epsilon\n",
    "        self.epsilon_decay = epsilon_decay #epsilon decay factor\n",
    "        self.final_epsilon = final_epsilon #minimum value of epsilon\n",
    "        self.discount_factor = discount_factor #gamma\n",
    "        \n",
    "        self.training_error = [] #list to store the training error at each episode\n",
    "        \n",
    "    def get_action(self, obs:tuple[int, int, bool])->int:\n",
    "        #epsilon-greedy policy, returns the action with the highest q-value \n",
    "        # for the given observation with probability 1-epsilon, this ensures exploration\n",
    "        if np.random.random() < self.epsilon: \n",
    "            return env.action_space.sample() #explore\n",
    "        else:\n",
    "            return np.argmax(self.q_values[obs]) #exploit\n",
    "\n",
    "    \n",
    "    def update(\n",
    "        self, obs:tuple[int, int, bool],\n",
    "        action:int,\n",
    "        reward:float,\n",
    "        next_obs:tuple[int, int, bool],\n",
    "        terminated:bool\n",
    "        )->None:\n",
    "        #update the q-values using the q-learning update rule \n",
    "        #and the agent's learning rate and discount factor\n",
    "        \n",
    "        # if the episode is terminated, the future q-value is 0 (no future rewards) if its not terminated, we compute the future q-value\n",
    "        future_q_value = 0 if terminated else np.max(self.q_values[next_obs])\n",
    "\n",
    "        temporal_difference = (\n",
    "            #compute the temporal difference (TD) error \n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "            )\n",
    "        \n",
    "        self.q_values[obs][action] = (\n",
    "            #update the q-value for the given observation and action\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "            )\n",
    "        \n",
    "        #append the TD error to the training error list\n",
    "        self.training_error.append(temporal_difference)\n",
    "        \n",
    "    def decay_epsilon(self)->None:\n",
    "        #decay the epsilon value\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.final_epsilon)\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100000\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / (n_episodes/2)\n",
    "final_epsilon = 0.1\n",
    "discount_factor = 0.95\n",
    "\n",
    "#initialize the agent  \n",
    "agent = BlackJackAgent(\n",
    "    epsilon=initial_epsilon,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=initial_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    discount_factor=discount_factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Only apply the wrapper once\n",
    "if not isinstance(env, gym.wrappers.RecordEpisodeStatistics):\n",
    "    # RecordEpisodeStatistics is a wrapper that keeps track of the rewards obtained in the last n episodes\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    clear_output()\n",
    "    \n",
    "    #play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs) #get the action\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action) #take the action and observe the results\n",
    "        agent.update(obs, action, reward, terminated, next_obs) #update the q-values\n",
    "        \n",
    "        #render the environment\n",
    "        frame = env.render() #render the environment\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        #plt.pause(0.01)\n",
    "        \n",
    "        obs = next_obs #update the observation\n",
    "        done = terminated or truncated #update the done flag\n",
    "    agent.decay_epsilon() #decay the epsilon value\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
