{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Tabular Solutions - **Solving the BlackJack environment**"
      ],
      "metadata": {
        "id": "y8x56c2U5FkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BlackJack** from gymnasium environments description"
      ],
      "metadata": {
        "id": "XmOwtupj5g2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blackjack is a card game where the goal is to beat the dealer by obtaining cards that sum to closer to 21 (without going over 21) than the dealers cards.\n",
        "\n",
        "The game starts with the dealer having one face up and one face down card, while the player has two face up cards. All cards are drawn from an infinite deck (i.e. with replacement).\n",
        "\n",
        "The card values are:\n",
        "\n",
        "- Face cards (Jack, Queen, King) have a point value of 10.\n",
        "\n",
        "- Aces can either count as 11 (called a ‘usable ace’) or 1.\n",
        "\n",
        "- Numerical cards (2-9) have a value equal to their number.\n",
        "\n",
        "The player has the sum of cards held. The player can request additional cards (hit) until they decide to stop (stick) or exceed 21 (bust, immediate loss).\n",
        "\n",
        "After the player sticks, the dealer reveals their facedown card, and draws cards until their sum is 17 or greater. If the dealer goes bust, the player wins.\n",
        "\n",
        "If neither the player nor the dealer busts, the outcome (win, lose, draw) is decided by whose sum is closer to 21.\n",
        "\n",
        "**Action Space**\n",
        "\n",
        "The action shape is (1,) in the range {0, 1} indicating whether to stick or hit.\n",
        "\n",
        "`Discrete(2)`\n",
        "\n",
        "- 0: Stick\n",
        "\n",
        "- 1: Hit\n",
        "\n",
        "**Observation Space**\n",
        "\n",
        "The observation consists of a 3-tuple containing: the player’s current sum, the value of the dealer’s one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1).\n",
        "\n",
        "The observation is returned as (int(), int(), int()).\n",
        "\n",
        "It is `Tuple(Discrete(32), Discrete(11), Discrete(2))`\n",
        "\n",
        "**Starting State**\n",
        "\n",
        "The starting state is initialised in the following range.\n",
        "\n",
        "| Observation                      | Min    | Max    |\n",
        "|----------------------------------|--------|--------|\n",
        "| Player current sum               | 4      | 12     |\n",
        "| Dealer showing card value        | 2      | 11     |\n",
        "| Usable Ace                       | 0      | 1      |\n",
        "\n",
        "\n",
        "**Rewards**\n",
        "\n",
        "- win game: +1\n",
        "\n",
        "- lose game: -1\n",
        "\n",
        "- draw game: 0\n",
        "\n",
        "- win game with natural blackjack: +1.5 (if natural is True) +1 (if natural is False)\n",
        "\n",
        "**Episode End**\n",
        "The episode ends if the following happens:\n",
        "\n",
        "1. Termination:\n",
        "\n",
        "The player hits and the sum of hand exceeds 21.\n",
        "\n",
        "2. The player sticks.\n",
        "\n",
        "An ace will always be counted as usable (11) unless it busts the player."
      ],
      "metadata": {
        "id": "ELIEBMH86SA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First approach: **Naïve Policy**"
      ],
      "metadata": {
        "id": "whufg_3e9F1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement an agent that carries out the following deterministic policy:\n",
        "\n",
        "- The agent will stick if it gets a score of 20 or 21.\n",
        "- Otherwise, it will hit.\n",
        "\n",
        "**Questions**:\n",
        "\n",
        "1. Using this agent, simulate 100,000 games and calculate the agent’s return(total accumulated reward).\n",
        "\n",
        "2. Additionally, calculate the % of wins, natural wins, losses and draws.\n",
        "\n",
        "3. Comment on the results."
      ],
      "metadata": {
        "id": "Sml_JgSs-aaL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koUx_EFK5ERJ"
      },
      "source": [
        "### Installing and importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6Rx18e-5ERN",
        "outputId": "22cd0815-00a9-42ac-eb7d-7dd8f67cd8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium==0.27.0\n",
            "  Downloading gymnasium-0.27.0-py3-none-any.whl (879 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m879.1/879.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.27.0) (1.23.5)\n",
            "Collecting jax-jumpy>=0.2.0 (from gymnasium==0.27.0)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.27.0) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.27.0) (4.5.0)\n",
            "Collecting gymnasium-notices>=0.0.1 (from gymnasium==0.27.0)\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from gymnasium==0.27.0)\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: gymnasium-notices, jax-jumpy, shimmy, gymnasium\n",
            "Successfully installed gymnasium-0.27.0 gymnasium-notices-0.0.1 jax-jumpy-1.0.0 shimmy-0.2.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "%pip install gymnasium#==0.27.0\n",
        "%pip install matplotlib\n",
        "%pip install numpy\n",
        "%pip install tqdm\n",
        "%matplotlib inline\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZTVFYS3BfXY",
        "outputId": "b63caf63-e788-4ef2-d15a-96ad1e5bd2ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0FZLhq4n5ERP"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict #for accessing keys which are not present in dictionary\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import random\n",
        "from matplotlib.patches import Patch\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExhcLlr5ERQ"
      },
      "source": [
        "### Crate the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "88YpPCxd5ERQ"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Blackjack-v1', sab=False, natural=True, render_mode='rgb_array') #We are not folllowing the default sutton and barto book settings, which are sab=True, natural=False, render_mode='human'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XU4d2zM5ERQ"
      },
      "source": [
        "Observe environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA5vmdXp5ERR",
        "outputId": "5641d80d-2796-4873-e480-f8cdee0aacc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "Action space: Discrete(2)\n",
            "Observation: (17, 8, 1)\n",
            "Info: {}\n"
          ]
        }
      ],
      "source": [
        "#observation space is a tuple of 3 elements:\n",
        "#1. player's current sum (1-31)\n",
        "#2. dealer's face up card (1-10)\n",
        "#3. whether or not the player has a usable ace (0 or 1)\n",
        "\n",
        "done = False\n",
        "observation, info = env.reset() # Reset the environment to get the first observation\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space) #0: stick, 1: hit\n",
        "print(\"Observation:\", observation) #player´s first two cards\n",
        "print(\"Info:\", info) #dealer´s first card\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5M2klB85ERS"
      },
      "outputs": [],
      "source": [
        "#env.step(action) returns: observation, reward, terminated, truncated, info\n",
        "\n",
        "#observation: tuple of 3 elements (player's current sum, dealer's face up card, whether or not the player has a usable ace)\n",
        "\n",
        "#reward: +1.5, +1, 0 or -1 (win, draw or loss), 1.5 if the player wins with a natural blackjack\n",
        "\n",
        "#terminated: boolean (True if the episode is over)\n",
        "\n",
        "#truncated: boolean (True if the episode is over because it reached the maximum number of steps)\n",
        "\n",
        "#info: dictionary with additional information. We will not use this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAGKi7rk5ERS",
        "outputId": "c375f06e-03de-47e7-eaac-98b888a5792e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random actions:\n",
            "Action: 1\n",
            "Observation: (23, 8, 0)\n",
            "Reward: -1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "\n",
            "Action: 1\n",
            "Observation: (13, 6, 0)\n",
            "Reward: 0.0\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "\n",
            "Action: 1\n",
            "Observation: (20, 2, 0)\n",
            "Reward: 0.0\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "\n",
            "Action: 1\n",
            "Observation: (17, 6, 0)\n",
            "Reward: 0.0\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "\n",
            "Action: 0\n",
            "Observation: (13, 3, 0)\n",
            "Reward: -1.0\n",
            "Terminated: True\n",
            "Truncated: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#sample random actions from the action space\n",
        "print(\"Random actions:\")\n",
        "for i in range(5):\n",
        "    env.reset() # reset the environment at the beginning of each iteration\n",
        "    action = env.action_space.sample()\n",
        "    print(\"Action:\", action)\n",
        "    observation, reward, terminated, truncated, info = env.step(action) #take a random action and observe the results of the action taken\n",
        "    print(\"Observation:\", observation)\n",
        "    print(\"Reward:\", reward)\n",
        "    print(\"Terminated:\", terminated)\n",
        "    print(\"Truncated:\", truncated)\n",
        "    #print(\"Info:\", info) Since there is no info in this environment we omit this\n",
        "    print(\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent"
      ],
      "metadata": {
        "id": "aPr7W-E3Eah0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBlackjackAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "id": "I9QpLX62ErRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UdYWzO85ERT"
      },
      "source": [
        "## Epsilon Greedy Strategy to solve blackjack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2_i1fWS5ERT"
      },
      "outputs": [],
      "source": [
        "class BlackJackAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        epsilon:float,\n",
        "        learning_rate:float,\n",
        "        initial_epsilon:float,\n",
        "        epsilon_decay:float,\n",
        "        final_epsilon:float,\n",
        "        discount_factor:float = 0.95,\n",
        "    ):\n",
        "        #initialize the agent's parameters with empty state-action value (q_values),\n",
        "        #a learning rate, an initial epsilon, an epsilon decay, a final epsilon and a discount factor\n",
        "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "        self.epsilon = epsilon #epsilon value\n",
        "        self.lr = learning_rate #learning rate\n",
        "        self.initial_epsilon = initial_epsilon #initial value of epsilon\n",
        "        self.epsilon_decay = epsilon_decay #epsilon decay factor\n",
        "        self.final_epsilon = final_epsilon #minimum value of epsilon\n",
        "        self.discount_factor = discount_factor #gamma\n",
        "\n",
        "        self.training_error = [] #list to store the training error at each episode\n",
        "\n",
        "    def get_action(self, obs:tuple[int, int, bool])->int:\n",
        "        #epsilon-greedy policy, returns the action with the highest q-value\n",
        "        # for the given observation with probability 1-epsilon, this ensures exploration\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return env.action_space.sample() #explore\n",
        "        else:\n",
        "            return np.argmax(self.q_values[obs]) #exploit\n",
        "\n",
        "\n",
        "    def update(\n",
        "        self, obs:tuple[int, int, bool],\n",
        "        action:int,\n",
        "        reward:float,\n",
        "        next_obs:tuple[int, int, bool],\n",
        "        terminated:bool\n",
        "        )->None:\n",
        "        #update the q-values using the q-learning update rule\n",
        "        #and the agent's learning rate and discount factor\n",
        "\n",
        "        # if the episode is terminated, the future q-value is 0 (no future rewards) if its not terminated, we compute the future q-value\n",
        "        future_q_value = 0 if terminated else np.max(self.q_values[next_obs])\n",
        "\n",
        "        temporal_difference = (\n",
        "            #compute the temporal difference (TD) error\n",
        "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
        "            )\n",
        "\n",
        "        self.q_values[obs][action] = (\n",
        "            #update the q-value for the given observation and action\n",
        "            self.q_values[obs][action] + self.lr * temporal_difference\n",
        "            )\n",
        "\n",
        "        #append the TD error to the training error list\n",
        "        self.training_error.append(temporal_difference)\n",
        "\n",
        "    def decay_epsilon(self)->None:\n",
        "        #decay the epsilon value\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.final_epsilon)\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzs6hAx15ERT"
      },
      "source": [
        "Training of the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8ycQXii5ERU"
      },
      "outputs": [],
      "source": [
        "#defining the hyperparameters\n",
        "learning_rate = 0.01\n",
        "n_episodes = 100000\n",
        "initial_epsilon = 1.0\n",
        "epsilon_decay = initial_epsilon / (n_episodes/2)\n",
        "final_epsilon = 0.1\n",
        "discount_factor = 0.95\n",
        "\n",
        "#initialize the agent\n",
        "agent = BlackJackAgent(\n",
        "    epsilon=initial_epsilon,\n",
        "    learning_rate=learning_rate,\n",
        "    initial_epsilon=initial_epsilon,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    final_epsilon=final_epsilon,\n",
        "    discount_factor=discount_factor\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIE5bybT5ERU"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from gymnasium.wrappers import RecordEpisodeStatistics\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Only apply the wrapper once\n",
        "if not isinstance(env, gym.wrappers.RecordEpisodeStatistics):\n",
        "    # RecordEpisodeStatistics is a wrapper that keeps track of the rewards obtained in the last n episodes\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    clear_output()\n",
        "\n",
        "    #play one episode\n",
        "    while not done:\n",
        "        action = agent.get_action(obs) #get the action\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action) #take the action and observe the results\n",
        "        agent.update(obs, action, reward, terminated, next_obs) #update the q-values\n",
        "\n",
        "        #render the environment\n",
        "        frame = env.render() #render the environment\n",
        "        plt.imshow(frame)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        #plt.pause(0.01)\n",
        "\n",
        "        obs = next_obs #update the observation\n",
        "        done = terminated or truncated #update the done flag\n",
        "    agent.decay_epsilon() #decay the epsilon value\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}