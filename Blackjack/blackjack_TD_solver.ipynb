{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (0.15.12)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (3.1.38)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (1.32.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: pathtools in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from wandb) (4.24.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (1.26.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pygame in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium==0.29.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from gymnasium==0.29.1) (1.26.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from gymnasium==0.29.1) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from gymnasium==0.29.1) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages (from gymnasium==0.29.1) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install tqdm\n",
    "%matplotlib inline\n",
    "%pip install pygame\n",
    "%pip install gymnasium==0.29.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "from collections import defaultdict #for accessing keys which are not present in dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import random\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJack_TD_Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon:float,\n",
    "        learning_rate:float,\n",
    "        initial_epsilon:float,\n",
    "        epsilon_decay:float,\n",
    "        final_epsilon:float,\n",
    "        discount_factor:float = 0.95,\n",
    "        env = gym.make('Blackjack-v1',sab=False, natural=True, render_mode='rgb_array')\n",
    "        ):\n",
    "        #initialize the agent's parameters with empty state-action value (q_values),\n",
    "        #a learning rate, an initial epsilon, an epsilon decay, a final epsilon and a discount factor\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        \n",
    "        self.epsilon = epsilon #epsilon value\n",
    "        self.lr = learning_rate #learning rate\n",
    "        self.initial_epsilon = initial_epsilon #initial value of epsilon\n",
    "        self.epsilon_decay = epsilon_decay #epsilon decay factor\n",
    "        self.final_epsilon = final_epsilon #minimum value of epsilon\n",
    "        self.discount_factor = discount_factor #gamma\n",
    "        self.env = env #environment\n",
    "        \n",
    "        self.training_error = [] #list to store the training error at each episode\n",
    "        \n",
    "    def get_action(self, obs:tuple[int, int, bool])->int:\n",
    "        #epsilon-greedy policy, returns the action with the highest q-value \n",
    "        # for the given observation with probability 1-epsilon, this ensures exploration\n",
    "        if np.random.random() < self.epsilon: \n",
    "            return self.env.action_space.sample() #explore\n",
    "        else:\n",
    "            return np.argmax(self.q_values[obs]) #exploit\n",
    "\n",
    "    \n",
    "    def update(\n",
    "        self, obs:tuple[int, int, bool],\n",
    "        action:int,\n",
    "        reward:float,\n",
    "        next_obs:tuple[int, int, bool],\n",
    "        terminated:bool\n",
    "        )->None:\n",
    "        #update the q-values using the q-learning update rule \n",
    "        #and the agent's learning rate and discount factor\n",
    "        \n",
    "        # if the episode is terminated, the future q-value is 0 (no future rewards) if its not terminated, we compute the future q-value\n",
    "        future_q_value = 0 if terminated else np.max(self.q_values[next_obs])\n",
    "\n",
    "        temporal_difference = (\n",
    "            #compute the temporal difference (TD) error \n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "            )\n",
    "        \n",
    "        self.q_values[obs][action] = (\n",
    "            #update the q-value for the given observation and action\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "            )\n",
    "        \n",
    "        #append the TD error to the training error list\n",
    "        self.training_error.append(temporal_difference)\n",
    "        \n",
    "    def decay_epsilon(self)->None:\n",
    "        #decay the epsilon value\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.final_epsilon)\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [14:02<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe7a6f063d0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fe7e5f62d10, execution_count=4 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fe7b2d42410, raw_cell=\"from collections import deque\n",
      "from gymnasium.wrapp..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/ndelafuente/Documents/Learn2Earn_RL/Blackjack/blackjack_TD_solver.ipynb#W3sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "from IPython.display import clear_output\n",
    "import wandb\n",
    "import pygame\n",
    "from numpy import random\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"blackjack_TD\", entity=\"ai42\")\n",
    "pygame.init()\n",
    "\n",
    "#defining the hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 10000\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / (n_episodes/2)\n",
    "final_epsilon = 0.1\n",
    "discount_factor = 0.95\n",
    "\n",
    "#load the environment\n",
    "env = gym.make('Blackjack-v1',sab=False, natural=True, render_mode='rgb_array') #We are not folllowing the default sutton and barto book settings, which are sab=True, natural=False, render_mode='human'\n",
    "\n",
    "#initialize the agent  \n",
    "agent = BlackJack_TD_Agent(\n",
    "    epsilon=initial_epsilon,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=initial_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    discount_factor=discount_factor,\n",
    "    env=env\n",
    ")\n",
    "\n",
    "wins = 0.0\n",
    "losses = 0.0\n",
    "draws = 0.0\n",
    "naturals = 0.0\n",
    "\n",
    "# Only apply the wrapper once\n",
    "if not isinstance(env, gym.wrappers.RecordEpisodeStatistics):\n",
    "    # RecordEpisodeStatistics is a wrapper that keeps track of the rewards obtained in the last n episodes\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    clear_output()\n",
    "    step = 0\n",
    "    episode_rewards = 0  # Initialize total rewards for the episode\n",
    "    \n",
    "    #play one episode\n",
    "    while not terminated and not truncated:\n",
    "        action = agent.get_action(obs) #get the action\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action) #take the action and observe the results\n",
    "        agent.update(obs, action, reward, terminated, next_obs) #update the q-values\n",
    "        \n",
    "        frame = env.render()\n",
    "        step += 1\n",
    "        episode_rewards += reward  # Accumulate rewards\n",
    "\n",
    "        # Plot frame\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Episode: {episode} - Step: {step} - Action Taken: {action} - Reward: {reward} - Terminated: {terminated}\")\n",
    "\n",
    "        plt.savefig('frame.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Log the frame and rewards to wandb\n",
    "        wandb.log({\n",
    "            \"episode\": episode,\n",
    "            \"step\": step,\n",
    "            \"frame\": wandb.Image('frame.png'),\n",
    "            \"reward\": reward,\n",
    "            \"cumulative_reward\": episode_rewards\n",
    "        })\n",
    "        \n",
    "        obs = next_obs #update the observation\n",
    "        \n",
    "    agent.decay_epsilon() #decay the epsilon value\n",
    "        \n",
    "    if reward == 1 or reward == 1.5:\n",
    "        wins += 1\n",
    "    elif reward == -1:\n",
    "        losses += 1\n",
    "    elif reward == 0:\n",
    "        draws += 1\n",
    "    if reward == 1.5:\n",
    "        naturals += 1\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# Let´s log general statistics of the training\n",
    "wandb.log({\"Win_rate\": wins / n_episodes, \"Loss_rate\": losses / n_episodes, \"Draw_rate\": draws / n_episodes, \"Natural_win_rate\": naturals / n_episodes}) # Log the episode statistics to wandb        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
