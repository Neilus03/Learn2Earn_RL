{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib.patches import Patch\n",
    "import os\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for training (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the QNet class\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, env, lr=0.005, device=device):\n",
    "        super(QNet, self).__init__()\n",
    "        \n",
    "        # Set device for training (GPU if available)\n",
    "        self.device = device\n",
    "        \n",
    "        # Get state and action space dimensions\n",
    "        self.state_space_dim = env.observation_space.shape[0]\n",
    "        self.action_space_dim = env.action_space.n\n",
    "        \n",
    "        # Define possible actions\n",
    "        self.actions = torch.arange(self.action_space_dim).to(device)\n",
    "        \n",
    "        # Set learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Define neural network architecture\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.state_space_dim, 16, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 32, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, self.action_space_dim, bias=True),\n",
    "            nn.Softmax(dim=-1),\n",
    "            )\n",
    "        \n",
    "        # Define optimizer\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        return self.net(x.to(self.device))\n",
    "    \n",
    "    \n",
    "    # Choose action based on epsilon-greedy policy\n",
    "    def act(self, state):\n",
    "        # Get action probabilities\n",
    "        action_probs = self.forward(state)\n",
    "        \n",
    "        # Choose action based on action probabilities \n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def update_policy(self, rewards, log_probs, gamma):\n",
    "        # Compute discounted rewards going backwards from the last state of the episode\n",
    "        discounted_rewards = []\n",
    "        R = 0\n",
    "        \n",
    "        for r in rewards[::-1]:    \n",
    "            # Reset reward sum if we encounter a non-zero reward\n",
    "            R = r + gamma * R\n",
    "            \n",
    "            # Insert discounted reward at the beginning of the list\n",
    "            discounted_rewards.insert(0, R)\n",
    "    \n",
    "        # Convert discounted rewards to tensor and pass to device\n",
    "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        #Normalize discounted rewards\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        #List to store policy gradient loss \n",
    "        policy_gradient_loss = []\n",
    "        \n",
    "        # Compute policy gradient loss for each step of the episode\n",
    "        for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "            \n",
    "            # append the negative of the policy gradient loss to the list\n",
    "            policy_gradient_loss.append(-log_prob * reward)\n",
    "        \n",
    "        #Stack the sum of the policy gradient losses of this episode to the list of policy gradient loss    \n",
    "        loss = torch.stack(policy_gradient_loss).sum()\n",
    "        \n",
    "        \n",
    "        # Reset gradients to zero\n",
    "        self.optimizer.zero_grad() \n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform the optimization step\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item() # Return the loss as a float for logging\n",
    "        \n",
    "    def save_model(self, filename):\n",
    "        # Save model to file\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load_model(self, filename, device='cuda'):\n",
    "        # Load model from file\n",
    "        self.load_state_dict(torch.load(filename, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', continuous=False)\n",
    "QNet = QNet(env).to(device)\n",
    "optimizer = optim.Adam(QNet.parameters(), lr=0.005)\n",
    "\n",
    "#HYPERPARAMETERS\n",
    "lr=0.005, \n",
    "gamma= 0.99, \n",
    "batch_size= 8,\n",
    "max_episodes= 5000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
