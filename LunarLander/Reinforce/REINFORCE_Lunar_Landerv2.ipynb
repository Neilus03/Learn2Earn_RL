{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib.patches import Patch\n",
    "import os\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for training (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the QNet class\n",
    "class REINFORCE(nn.Module):\n",
    "    def __init__(self, env, lr=0.005, device=device):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        \n",
    "        # Set device for training (GPU if available)\n",
    "        self.device = device\n",
    "        \n",
    "        # Get state and action space dimensions\n",
    "        self.state_space_dim = env.observation_space.shape[0]\n",
    "        self.action_space_dim = env.action_space.n\n",
    "        \n",
    "        # Define possible actions\n",
    "        self.actions = torch.arange(self.action_space_dim).to(device)\n",
    "        \n",
    "        # Set learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Define neural network architecture\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.state_space_dim, 16, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 32, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, self.action_space_dim, bias=True),\n",
    "            nn.Softmax(dim=-1),\n",
    "            )\n",
    "        \n",
    "        # Define optimizer\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        # Define lr scheduler to decrease learning rate every 100 episodes (optional)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        return self.net(x.to(self.device))\n",
    "    \n",
    "    \n",
    "    # Choose action based on epsilon-greedy policy\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        state: current state of the environment\n",
    "        \n",
    "        The act method takes in a state and returns an action based on the policy.\n",
    "        It first gets the action probabilities from the network, then chooses an action based on these probabilities.\n",
    "        '''\n",
    "        # Get action probabilities\n",
    "        action_probs = self.forward(state)\n",
    "        \n",
    "        # Choose action based on action probabilities \n",
    "        #torch's multinomial function returns a tensor of size 1 containing the index of the action chosen\n",
    "        #it works by sampling from the multinomial distribution defined by the action probabilities\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def update_policy(self, rewards, log_probs, gamma):\n",
    "        '''\n",
    "        Rewards: list of rewards from the most recent episode\n",
    "        Log_probs: list of log probabilities for the actions taken in the most recent episode\n",
    "        Gamma: discount factor\n",
    "        \n",
    "        \n",
    "        The update_policy method takes in a list of rewards and log probabilities, and a discount factor gamma.\n",
    "        It then calculates the policy gradient loss and backpropagates it through the network.\n",
    "        Finally, it steps the optimizer and returns the loss as a float.\n",
    "        \n",
    "        '''\n",
    "        # Convert rewards to a PyTorch tensor and send to the appropriate device\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # we flip the rewards tensor, using torch.flip() to get the cumulative sum in the reverse direction\n",
    "        # making the short term rewards have a higher weight than the long term rewards\n",
    "        rewards = rewards.flip(dims=(0,))\n",
    "        \n",
    "        \n",
    "        #pytorch's cumsum computes the cumulative sum of the elements along a given axis, in this case the rewards\n",
    "        #we multiply the rewards by gamma to the power of their index, which is the same as multiplying by gamma\n",
    "        discounted_rewards = torch.cumsum(rewards * gamma, dim=0)\n",
    "        \n",
    "        #Now we flip the rewards tensor back to its original order\n",
    "        discounted_rewards = discounted_rewards.flip(dims=(0,))\n",
    "        \n",
    "        # Normalize discounted rewards (substract mean and divide by standard deviation)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        # Stack the log probabilities into a tensor\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        # Calculate the policy gradient loss:\n",
    "        # We use the negative log probabilities to get the log probabilities of the actions that were actually taken\n",
    "        # We then multiply these log probabilities by the discounted rewards to get the policy gradient loss\n",
    "        policy_gradient_loss = -log_probs * discounted_rewards \n",
    "        \n",
    "        # Sum the policy gradient loss over all timesteps\n",
    "        loss = policy_gradient_loss.sum()\n",
    "        \n",
    "        # Reset gradients to zero before backpropagation beacuse PyTorch accumulates gradients\n",
    "        self.optimizer.zero_grad() #reset the gradients of the scheduler (the optimizer) to zero\n",
    "        \n",
    "        # Backpropagate the loss \n",
    "        loss.backward()\n",
    "        \n",
    "        # Optionally clip gradients to prevent large updates, this works by scaling the gradients if their norm is larger than 1\n",
    "        torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "        \n",
    "        # Step the optimizer to update the network parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Return the loss as a float for logging purposes\n",
    "        return loss.item()  \n",
    "\n",
    "        \n",
    "    def save_model(self, filename):\n",
    "        # Save model to file\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load_model(self, filename, device='cuda'):\n",
    "        # Load model from file\n",
    "        self.load_state_dict(torch.load(filename, map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxiliary functions\n",
    "\n",
    "# Function to preprocess states\n",
    "def preprocess_state(state):\n",
    "    # Convert state to a numpy array\n",
    "    return torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "# Function to clear the video directory\n",
    "def clear_video_directory(video_dir):\n",
    "    # Get list of video files in video directory \n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "    # Delete all video files in video directory\n",
    "    for file in video_files:\n",
    "        os.remove(os.path.join(video_dir, file))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneildlf\u001b[0m (\u001b[33mai42\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ndelafuente/Desktop/Learn2Earn_RL/LunarLander/Reinforce_Lunar_Lander/wandb/run-20231121_133524-bwvsafq9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai42/reinforce_lunar_lander/runs/bwvsafq9' target=\"_blank\">rich-universe-28</a></strong> to <a href='https://wandb.ai/ai42/reinforce_lunar_lander' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai42/reinforce_lunar_lander' target=\"_blank\">https://wandb.ai/ai42/reinforce_lunar_lander</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai42/reinforce_lunar_lander/runs/bwvsafq9' target=\"_blank\">https://wandb.ai/ai42/reinforce_lunar_lander/runs/bwvsafq9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ndelafuente/miniconda3/envs/pytorch-env/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/ndelafuente/Desktop/Learn2Earn_RL/LunarLander/Reinforce_Lunar_Lander/Videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ndelafuente/Desktop/Learn2Earn_RL/LunarLander/Reinforce_Lunar_Lander/Videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ndelafuente/Desktop/Learn2Earn_RL/LunarLander/Reinforce_Lunar_Lander/Videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ndelafuente/Desktop/Learn2Earn_RL/LunarLander/Reinforce_Lunar_Lander/Videos/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average Reward: -31.517590767269127, Loss: 0.37171459197998047\n",
      "Episode 100, Average Reward: -156.41221147461295, Loss: -0.28967952728271484\n",
      "Episode 200, Average Reward: -144.5927366673507, Loss: -5.400518894195557\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"reinforce_lunar_lander\", entity=\"ai42\")\n",
    "\n",
    "#HYPERPARAMETERS\n",
    "lr=5e-3 # learning rate\n",
    "gamma= 0.99 # discount factor\n",
    "max_episodes= 5000  # max number of episodes to learn from\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = gym.make('LunarLander-v2', continuous=False, render_mode='rgb_array')\n",
    "reinforce_agent = REINFORCE(env).to(device)\n",
    "\n",
    "\n",
    "\n",
    "best_score = -np.inf\n",
    "\n",
    "# Define a video directory\n",
    "video_dir = '/home/ndelafuente/Desktop/Learn2Earn_RL/LunarLander/Reinforce_Lunar_Lander/Videos' # Set this to your preferred directory\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Wrap your environment to record videos\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=video_dir, episode_trigger=lambda episode_id: True)\n",
    "\n",
    "# Set up lists to hold results\n",
    "all_rewards = []\n",
    "avg_rewards = []\n",
    "losses = []\n",
    "\n",
    "# Set up flag to indicate when to record a video of the agent playing\n",
    "record_video = False\n",
    "\n",
    "# Replace 'max_episodes' with the actual number\n",
    "for episode in range(max_episodes): \n",
    "        \n",
    "    state, info = env.reset()\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    total_reward = 0\n",
    "\n",
    "    #If the last episode was the best so far, we record a video of the agent playing\n",
    "    if record_video:\n",
    "        # Wrap environment to record videos if the last episode was the best so far\n",
    "        env = gym.wrappers.RecordVideo(env, video_folder=video_dir, episode_trigger=lambda episode_id: True)\n",
    "        \n",
    "    # Run episode\n",
    "    for t in range(env.spec.max_episode_steps):  # Limit the number of timesteps per episode: env.spec.max_episode_steps by default is 1000\n",
    "        \n",
    "        # Get action from agent \n",
    "        action = reinforce_agent.act(state)\n",
    "        \n",
    "        # Take step in environment \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Preprocess next state\n",
    "        next_state = preprocess_state(next_state)\n",
    "        \n",
    "        #we use the log_prob method of the distribution to get the log probability of the action taken in the current state \n",
    "        log_prob = torch.log(reinforce_agent.forward(state)[0][action])\n",
    "        \n",
    "        #we append the log probability and the reward to their respective lists\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        #update the total reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if the episode is terminated or truncated (the lander has crashed or landed or the episode has reached the maximum number of timesteps)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "     \n",
    "    \n",
    "    # Calculate loss using the reinforce_agent.update_policy() method \n",
    "    loss = reinforce_agent.update_policy(rewards, log_probs, gamma)\n",
    "    \n",
    "    # Append loss to list\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Append total reward to list\n",
    "    all_rewards.append(total_reward)\n",
    "    \n",
    "    # Calculate average reward over the last 100 episodes and append to list\n",
    "    avg_reward = np.mean(all_rewards[-100:])  \n",
    "    avg_rewards.append(avg_reward)\n",
    "\n",
    "    # Save the model if it has improved\n",
    "    if avg_reward > best_score:\n",
    "        reinforce_agent.save_model('best_model.pth')\n",
    "        best_score = avg_reward\n",
    "        \n",
    "    if reward > best_score: #check if episode reward is better than best score so far \n",
    "        record_video = True\n",
    "    \n",
    "    #After the episode is finished, remove the video recording wrapper for the next episode\n",
    "    if isinstance(env, gym.wrappers.RecordVideo):\n",
    "        # Get list of video files in video directory\n",
    "        video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "        if video_files:\n",
    "            # Log the most recent video file to Weights & Biases \n",
    "            last_video_file = video_files[-1]  # The most recent video file\n",
    "            wandb.log({\"episode_video\": wandb.Video(os.path.join(video_dir, last_video_file), fps=4, format=\"mp4\")})\n",
    "        \n",
    "        env = env.env\n",
    "    \n",
    "    # Log results to Weights & Biases for tracking\n",
    "    wandb.log({\"Episode Reward\": total_reward, \"Average Reward\": avg_reward, \"Loss\": loss, \"Episode\": episode})   \n",
    "    \n",
    "    # Clear videos directory \n",
    "    clear_video_directory(video_dir)\n",
    "\n",
    "    # Print progress\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Average Reward: {avg_reward}, Loss: {loss}\")\n",
    "\n",
    "\n",
    "# Close environment and Weights & Biases\n",
    "env.close()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
